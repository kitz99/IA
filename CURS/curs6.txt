============= Curs 6 =============
==         24.03.2014           ==
==================================

-> Cursul vizeaza a doua categorie de perceptroni, si anume aceaaa ce au functie de transfer liniara

   PERCEPTRONUL CU FUNCTE DE TRANSFER LINIARA

   		<insert figura 2-3 - Adaptive liniar combiner element >

   		-> Acest perceptron in spatiul ipotezelor, gaseste acea ipoteza, ai pe multimea functiei de antrenare, media functiei de cost sa fie minima;

   		-> Algoritmul de antrenare ptr un astfel de perceptron a fost propus de catre Widrow si Hoff

   		*** Algoritmul de Antrenare ***
   		    <insert fig 2.30>
        
        A doua problema pentru a gasi proprietatile acestei reguli de invatare este problema gasirii functiei de pierdere. Formula 2.32 spune ca aceasta evaluare reprezinta suma patratelor erorilor modulo 1/2.

        -> functie de cost reprez diferenta dintre respunsul corect si cel dat de perceptron;

        -> daca aplicam metoda gradientului descendent folosind aceasta functie criteriu, observam ca reprezinta marja de incredere pentru acest model; (2.34)

        -> formula 2.35 reprezinta aceeasi formula, doar ca in forma online(actualizarea are loc pas cu pas);

        -> Rata de invatare depinde de exemplul k (2.36)

        -> rezultatul dat de 2.37 raspunde la intrebarea : "in ce interval tre sa aleg rata de invatare pentru ca alg meu, daca are solutie, va converge. [voi gasi acei w care det dreapta ce separa multimea]". rata de invatare tre sa fie pozitiva

        -> solutia catre care converege algoritmul este data de formula 2.38 ~~~> Formula analitica

        -> Algoritmul acesta este singurul pentru care exista o solutie unica si analitica

        *********** DEMONSTRATIA FORMULEI 2.38 ******************** [s-a insistat mult pe formula asta]

        	-> X -> matricea exemplelor
        	-> x -> vector nx1 - corespunde intrarilor perceptronului;
        	-> X - matrice dreptunghiulara (mxn) -> nr de exemple este mai mare decat variabilele masurate asuptra exemplului x
        	-> d reptrezinta un vector m*1 ce contine valorile reale (etichete);
        	-> X' - inversa generalizata (X-ul cu semnul acela dubios);

@@@ Truc care se retine dupa examen: Inversa unei matrice dreptunghiulare:
					-> produsul intre  X si X^T (=> matrice patratica ptr care ne putem pune problema inversabilitatii)
					-> matricea obtinuta este simetrica si elementele de pe diagonala principala sunt majorante (nu exista pe linia resp un element mai mare ca el) - deci foarte probabil inversabila [numeric inversabila]
			
			-> In continuarea demonstratiei se calculeaza punctele stationare ale functiei J(x) care se afla printre solutiile ecuatiei 2.39;

			-> Se obtine prin derivare formula 2.40;

			-> Din 2.40 se poate obtine 2.41

			-> inmultim cu XX^T si obtinem formula 2.42.

			-> pentru a incheia demonstratia trebuie sa spunem ce fel de punct este acest w* prin calcului derivatei a doua (hesiana) [vezi pag 1 din curs];

			-> calculand efecticv derivata observam ca hessiana este negativa => "tine apa";

			-> de remarcat ca w* exista indiferent daca mutimile sunt sau nu sunt liniar separabile. daca multimile sunt liniar separabile, atunci w* va determina un hiperplan care va separa punctele fara eroare. Altfel, hiperplanul se va det in asa vel in cat erarea sa fie minima;

			->
 





